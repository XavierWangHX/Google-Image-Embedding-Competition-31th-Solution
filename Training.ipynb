{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2778fde9-ad4b-4ece-8af8-3cc75603dc13",
   "metadata": {},
   "source": [
    "#### General_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbf8845f-c37a-4219-acb9-2120b78a586a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "  logger.warn(\n",
      "The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run\n",
      "WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.\n",
      " Please read local_rank from `os.environ('LOCAL_RANK')` instead.\n",
      "INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:\n",
      "  entrypoint       : /root/image_embedding/train.py\n",
      "  min_nodes        : 1\n",
      "  max_nodes        : 1\n",
      "  nproc_per_node   : 1\n",
      "  run_id           : none\n",
      "  rdzv_backend     : static\n",
      "  rdzv_endpoint    : 127.0.0.1:29500\n",
      "  rdzv_configs     : {'rank': 0, 'timeout': 900}\n",
      "  max_restarts     : 3\n",
      "  monitor_interval : 5\n",
      "  log_dir          : None\n",
      "  metrics_cfg      : {}\n",
      "\n",
      "INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_0d9sqg0h/none_7k3b1g9b\n",
      "INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python\n",
      "INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.\n",
      "  warnings.warn(\n",
      "INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\n",
      "  restart_count=0\n",
      "  master_addr=127.0.0.1\n",
      "  master_port=29500\n",
      "  group_rank=0\n",
      "  group_world_size=1\n",
      "  local_ranks=[0]\n",
      "  role_ranks=[0]\n",
      "  global_ranks=[0]\n",
      "  role_world_sizes=[1]\n",
      "  global_world_sizes=[1]\n",
      "\n",
      "INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\n",
      "INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_0d9sqg0h/none_7k3b1g9b/attempt_0/0/error.json\n",
      "\u001b[32m[2022-10-18 13:52:14 CLIP_Vit]\u001b[0m\u001b[33m(train.py 235)\u001b[0m: INFO BASE:\n",
      "- ''\n",
      "IMG_SIZE: 224\n",
      "MODEL:\n",
      "  Embedding_dim: 64\n",
      "  NAME: CLIP_Vit\n",
      "  backbone:\n",
      "    VIT:\n",
      "      heads: 16\n",
      "      image_size: 224\n",
      "      layers: 40\n",
      "      mlp_ratio: 4.3637\n",
      "      output_dim: 1024\n",
      "      patch_size: 14\n",
      "      width: 1408\n",
      "    pretrained: pretrained_models/ViT_G_14_2B_vision_model.pt\n",
      "  finetune: null\n",
      "  head:\n",
      "    name: Arc_face\n",
      "  num_classes: 27704\n",
      "  output_dir: /root/autodl-tmp/vit_Gaint\n",
      "Optimizer:\n",
      "  weight_decay: 1.0e-05\n",
      "SEED: 3407\n",
      "\n",
      "\u001b[32m[2022-10-18 13:52:29 CLIP_Vit]\u001b[0m\u001b[33m(build_model.py 35)\u001b[0m: INFO => Load pretrained vit_backbone 'pretrained_models/ViT_G_14_2B_vision_model.pt' successfully\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "\u001b[32m[2022-10-18 13:52:33 CLIP_Vit]\u001b[0m\u001b[33m(train.py 189)\u001b[0m: INFO Start training\n",
      "\u001b[32m[2022-10-18 13:52:33 CLIP_Vit]\u001b[0m\u001b[33m(train.py 194)\u001b[0m: INFO ----------[Epoch 1]----------\n",
      "  0%|                                                 | 0/29098 [00:00<?, ?it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Loss_cur: 25.93834, Loss_avg: 25.93834:   0%| | 1/29098 [00:02<24:09:41,  2.99s/Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Loss_cur: 25.11846, Loss_avg: 25.52840:   0%| | 2/29098 [00:03<12:27:49,  1.54s/Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Loss_cur: 25.87647, Loss_avg: 25.46852:   0%| | 28/29098 [00:17<4:17:32,  1.88itGradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "Loss_cur: 25.45444, Loss_avg: 25.25564:   0%| | 99/29098 [00:55<4:20:58,  1.85it\u001b[32m[2022-10-18 13:53:29 CLIP_Vit]\u001b[0m\u001b[33m(train.py 129)\u001b[0m: INFO Epoch: 1 | Iter: [100/29098], lr: 0.0000103, Memory_used: 7820MB, loss_cur: 25.45444, loss_avg: 25.25564, batch_time_avg: 0.560, time_total: 55.958\n",
      "Loss_cur: 24.98894, Loss_avg: 25.25300:   0%| | 101/29098 [00:56<4:21:07,  1.85i^C\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 \\\n",
    "python -m torch.distributed.launch --nproc_per_node=1 \\\n",
    "/root/image_embedding/train.py \\\n",
    "--csv-dir new_data_224_mini_fold.csv \\\n",
    "--config-name 'vit_224' \\\n",
    "--image-size 224 \\\n",
    "--batch-size 32 \\\n",
    "--num-workers 8 \\\n",
    "--init-lr 1e-4 \\\n",
    "--n-epochs 50 \\\n",
    "--arc_s 30.0 \\\n",
    "--arc_m 0.30 \\\n",
    "--n_batch_log 100 \\\n",
    "--warm_up_epochs 1 \\\n",
    "--fold 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc692256-2789-4cf8-a5ba-ca7d26d038df",
   "metadata": {},
   "source": [
    "### Get Visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850f90b4-e7a9-4ea4-8d3e-24f1a1fb6b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "import torch\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-g-14', pretrained='laion2b_s12b_b42k', cache_dir='pretrained_models')\n",
    "model_visual = model.visual\n",
    "torch.save(model_visual.state_dict(), 'pretrained_models/ViT_G_14_2B_vision_model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
